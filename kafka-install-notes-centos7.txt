#\\\\\\\\\\\\\\\\\\\\
# Kafka on CentOS 7
# May 2021
# Kafka Portion Based On: https://www.digitalocean.com/community/tutorials/how-to-install-apache-kafka-on-centos-7
# *do not use this for a production deployment*
#////////////////////


# this project needs at least 4 GB RAM


#=============
# STEP 0 - INSTALL JAVA 8
#=============
# if no network connectivity
# sudo dhclient ens33
sudo yum install java-1.8.0-openjdk -y


#=============
# STEP 1 - CREATE THEN USE KAFKA USER
#=============
sudo useradd kafka -ms /bin/bash -G wheel
sudo passwd kafka
su -l kafka


#=============
# STEP 2 - DOWNLOAD BINARIES
#=============
mkdir ~/Downloads
curl "https://mirrors.ocf.berkeley.edu/apache/kafka/2.8.0/kafka_2.13-2.8.0.tgz" -o ~/Downloads/kafka.tgz
mkdir ~/kafka && cd ~/kafka
tar -xvzf ~/Downloads/kafka.tgz --strip 1


#=============
# STEP 3 - CONFIGURE SERVER TO ALLOW DELETES
#=============
echo "delete.topic.enable = true" >> ~/kafka/config/server.properties


#=============
# STEP 4 - CREATE SYSTEMD UNIT FILES AND START THE SERVER
#=============
sudo nano /etc/systemd/system/zookeeper.service
#######
[Unit]
Requires=network.target remote-fs.target
After=network.target remote-fs.target

[Service]
Type=simple
User=kafka
ExecStart=/home/kafka/kafka/bin/zookeeper-server-start.sh /home/kafka/kafka/config/zookeeper.properties
ExecStop=/home/kafka/kafka/bin/zookeeper-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
#######


sudo nano /etc/systemd/system/kafka.service
#######
[Unit]
Requires=zookeeper.service
After=zookeeper.service

[Service]
Type=simple
User=kafka
ExecStart=/bin/sh -c '/home/kafka/kafka/bin/kafka-server-start.sh /home/kafka/kafka/config/server.properties > /home/kafka/kafka/kafka.log 2>&1'
ExecStop=/home/kafka/kafka/bin/kafka-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
#######

sudo systemctl start kafka
sudo journalctl -u kafka
sudo systemctl enable kafka


#=============
# STEP 5 - TEST THE INSTALLATION
#=============
# open two terminal windows, both logged in as the kafka user

# terminal 1 - create a topic then send a message
~/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic TutorialTopic
echo "Hello, World" | ~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic TutorialTopic > /dev/null

# terminal 2 - consume the message
~/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic TutorialTopic --from-beginning
# re-send messages in terminal 1 as desired to test


#=============
# STEP 6 - INSTALL KafkaT
# Airbnb tool for easier viewing of cluster details
#=============
sudo yum install ruby ruby-devel make gcc patch
sudo gem install kafkat

nano ~/.kafkatcfg
#######
{
	"kafka_path": "~/kafka",
	"log_path": "/tmp/kafka-logs",
	"zk_path": "localhost:2181"
}
#######

kafkat partitions


#=============
# STEP 7 - OPTIONAL - SETTING UP A MULTI-NODE CLUSTER
#=============
If you want to create a multi-broker cluster using more Ubuntu 18.04 machines, you should repeat Step 1, Step 4, and Step 5 on each of the new machines. Additionally, you should make the following changes in the server.properties file for each:
	- The value of the broker.id property should be changed such that it is unique throughout the cluster. This property uniquely identifies each server in the cluster and can have any string as its value. For example, "server1", "server2", etc.
	- The value of the zookeeper.connect property should be changed such that all nodes point to the same ZooKeeper instance. This property specifies the Zookeeper instanceâ€™s address and follows the <HOSTNAME/IP_ADDRESS>:<PORT> format. For example, "203.0.113.0:2181", "203.0.113.1:2181" etc.
If you want to have multiple ZooKeeper instances for your cluster, the value of the zookeeper.connect property on each node should be an identical, comma-separated string listing the IP addresses and port numbers of all the ZooKeeper instances.


#=============
# STEP 8 - RESTRICT KAFKA USER
#=============
# switch back to non-kafka superuser
# remove kafka from sudo group and lock password
#	- only root or sudo user can log in with "sudo su - kafka"
#	- root can add back to sudo with "usermod -aG wheel kafka"
#	- to unlock, use "sudo passwd kafka -u"
sudo gpasswd -d kafka wheel
sudo passwd kafka -l


#=============
# TEST USING BEATS (PRODUCERS) AND LOGSTASH (CONSUMER)
#=============
cd ~/Downloads
wget https://artifacts.elastic.co/downloads/logstash/logstash-7.10.2-x86_64.rpm
wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.10.2-x86_64.rpm
wget https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-7.10.2-x86_64.rpm
wget https://artifacts.elastic.co/downloads/beats/packetbeat/packetbeat-7.10.2-x86_64.rpm
sudo yum localinstall logstash-7.10.2-x86_64.rpm
sudo yum localinstall filebeat-7.10.2-x86_64.rpm
sudo yum localinstall metricbeat-7.10.2-x86_64.rpm
sudo yum localinstall packetbeat-7.10.2-x86_64.rpm

# set output statements to push ("publish") into Kafka cluster
#######
output.kafka:
  enabled: true
  hosts: ["localhost:9092"]
  topic: "filebeat"
#######
output.kafka:
  enabled: true
  hosts: ["localhost:9092"]
  topic: "metricbeat"
#######
output.kafka:
  enabled: true
  hosts: ["localhost:9092"]
  topic: "packetbeat"
#######

# Logstash input statement to read ("consume") from Kafka cluster
#######
input {
	kafka {
		bootstrap_servers => "localhost:9092"
		topics => ["filebeat", "metricbeat", "packetbeat"]
	}
}
#######


#=============
# FURTHER TESTING - SET FIREWALL RULES, SEND REMOTE TEST SERVER DATA INTO CLUSTER
# THIS IS NOT FOR PRODUCTION
#=============

sudo systemctl start firewalld
sudo systemctl enable firewalld
sudo firewall-cmd --zone=public --add-port=2181/tcp --permanent
sudo firewall-cmd --zone=public --add-port=2888/tcp --permanent
sudo firewall-cmd --zone=public --add-port=3888/tcp --permanent
sudo firewall-cmd --zone=public --add-port=9092/tcp --permanent
sudo firewall-cmd --reload
sudo iptables-save | grep "2181|2888|3888|9092"

