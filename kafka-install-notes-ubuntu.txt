#\\\\\\\\\\\\\\\\\\\\
# Kafka on Ubuntu 18.04
# March 2021
# Kafka Portion Based On: https://www.digitalocean.com/community/tutorials/how-to-install-apache-kafka-on-ubuntu-18-04
# *do not use this for a production deployment*
#////////////////////


#=============
# STEP 0 - INSTALL JAVA 8
#=============
sudo apt install openjdk-8-jdk openjdk-8-jre -qy


#=============
# STEP 1 - CREATE THEN USE KAFKA USER
#=============
sudo useradd kafka -ms /bin/bash -G sudo
sudo passwd kafka
su -l kafka


#=============
# STEP 2 - DOWNLOAD BINARIES
#=============
mkdir ~/Downloads
curl "https://mirrors.ocf.berkeley.edu/apache/kafka/2.7.0/kafka_2.13-2.7.0.tgz" -o ~/Downloads/kafka.tgz
mkdir ~/kafka && cd ~/kafka
tar -xvzf ~/Downloads/kafka.tgz --strip 1


#=============
# STEP 3 - CONFIGURE SERVER TO ALLOW DELETES
#=============
echo "delete.topic.enable = true" >> ~/kafka/config/server.properties


#=============
# STEP 4 - CREATE SYSTEMD UNIT FILES AND START THE SERVER
#=============
sudo nano /etc/systemd/system/zookeeper.service
#######
[Unit]
Requires=network.target remote-fs.target
After=network.target remote-fs.target

[Service]
Type=simple
User=kafka
ExecStart=/home/kafka/kafka/bin/zookeeper-server-start.sh /home/kafka/kafka/config/zookeeper.properties
ExecStop=/home/kafka/kafka/bin/zookeeper-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
#######

sudo nano /etc/systemd/system/kafka.service
#######
[Unit]
Requires=zookeeper.service
After=zookeeper.service

[Service]
Type=simple
User=kafka
ExecStart=/bin/sh -c '/home/kafka/kafka/bin/kafka-server-start.sh /home/kafka/kafka/config/server.properties > /home/kafka/kafka/kafka.log 2>&1'
ExecStop=/home/kafka/kafka/bin/kafka-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
#######

sudo systemctl start kafka
sudo journalctl -u kafka
sudo systemctl enable kafka


#=============
# STEP 5 - TEST THE INSTALLATION
#=============
# open two terminal windows, both logged in as the kafka user

# terminal 1 - create a topic then send a message
~/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic TutorialTopic
echo "Hello, World" | ~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic TutorialTopic > /dev/null

# terminal 2 - consume the message
~/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic TutorialTopic --from-beginning
# re-send messages in terminal 1 as desired to test


#=============
# STEP 6 - INSTALL KafkaT
# Airbnb tool for easier viewing of cluster details
#=============
sudo apt install ruby ruby-dev build-essential -qy
sudo gem install kafkat

nano ~/.kafkatcfg
#######
{
	"kafka_path": "~/kafka",
	"log_path": "/tmp/kafka-logs",
	"zk_path": "localhost:2181"
}
#######

kafkat partitions


#=============
# STEP 7 - OPTIONAL - SETTING UP A MULTI-NODE CLUSTER
#=============
If you want to create a multi-broker cluster using more Ubuntu 18.04 machines, you should repeat Step 1, Step 4, and Step 5 on each of the new machines. Additionally, you should make the following changes in the server.properties file for each:
	- The value of the broker.id property should be changed such that it is unique throughout the cluster. This property uniquely identifies each server in the cluster and can have any string as its value. For example, "server1", "server2", etc.
	- The value of the zookeeper.connect property should be changed such that all nodes point to the same ZooKeeper instance. This property specifies the Zookeeper instanceâ€™s address and follows the <HOSTNAME/IP_ADDRESS>:<PORT> format. For example, "203.0.113.0:2181", "203.0.113.1:2181" etc.
If you want to have multiple ZooKeeper instances for your cluster, the value of the zookeeper.connect property on each node should be an identical, comma-separated string listing the IP addresses and port numbers of all the ZooKeeper instances.


#=============
# STEP 8 - RESTRICT KAFKA USER
#=============
# switch back to non-kafka superuser
# remove kafka from sudo group and lock password
#	- only root or sudo user can log in with "sudo su - kafka"
#	- to unlock, use "sudo passwd kafka -u"
sudo deluser kafka sudo
sudo passwd kafka -l


#=============
# STEP 9 - TEST AGAIN
#=============
# open two terminal windows, both logged in as the kafka user

# terminal 1 - create a topic then send a message
~/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic TutorialTopic
echo "Hello, World" | sudo /home/kafka/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic TutorialTopic > /dev/null

# terminal 2 - consume the message
sudo /home/kafka/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic TutorialTopic --from-beginning
# re-send messages in terminal 1 as desired to test


#=============
# TEST USING BEATS (PRODUCERS) AND LOGSTASH (CONSUMER)
#=============
cd ~/Downloads
wget https://artifacts.elastic.co/downloads/logstash/logstash-7.10.2-amd64.deb
wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.10.2-amd64.deb
wget https://artifacts.elastic.co/downloads/beats/packetbeat/packetbeat-7.10.2-amd64.deb
wget https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-7.10.2-amd64.deb
# install all to /etc/
sudo dpkg -i logstash-7.10.2-amd64.deb
sudo dpkg -i filebeat-7.10.2-amd64.deb
sudo dpkg -i metricbeat-7.10.2-amd64.deb
sudo dpkg -i packetbeat-7.10.2-amd64.deb


#=============
# FILEBEAT - READ LOGS AND SEND TO KAFKA
#=============
# for testing, do not enable any modules unless desired
sudo nano /etc/filebeat/filebeat.yml
#######
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/*.log
filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false
tags: ["kbox", "kafka", "test"]
output.kafka:
  enabled: true
  hosts: ["kbox:9092"]
  topic: "filebeat"
#######

#sudo systemctl enable filebeat # only enable if start-on-boot is desired
sudo systemctl start filebeat
sudo systemctl status filebeat
# in another terminal
sudo /home/kafka/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic filebeat --from-beginning


#=============
# PACKETBEAT - READ NETWORK TRAFFIC AND SEND TO KAFKA
#=============
# for testing, do not enable any modules unless desired
sudo nano /etc/packetbeat/packetbeat.yml
# add the lines below, and remove the "output.elasticsearch" line
#######
output.kafka:
  enabled: true
  hosts: ["kbox:9092"]
  topic: "packetbeat"
#######

#sudo systemctl enable packetbeat # only enable if start-on-boot is desired
sudo systemctl start packetbeat
sudo systemctl status packetbeat
# in another terminal
sudo /home/kafka/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic packetbeat --from-beginning


#=============
# LOGSTASH - READ FROM KAFKA AND SEND TO FILE / STDOUT
#=============
sudo nano /etc/logstash/conf.d/kafka-ingest.conf
#######
input {
        kafka {
                bootstrap_servers => "localhost:9092"
                topics => ["filebeat", "packetbeat"]
        }
}
filter {
	json {
		source => "message"
	}
}
output {
        stdout { codec => rubydebug }
        file { path => "/var/log/logstash/logstash-test.log" }
}
#######

# test the config
sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/ --config.test_and_exit
# run Logstash; will print events to stdout
sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/ --config.reload.automatic
# in another terminal
sudo tail -f /var/log/logstash/logstash-test.log | jq '.'

# sudo /usr/share/logstash/bin/system-install # only enable if start-on-boot is desired
# sudo systemctl enable logstash # only enable if start-on-boot is desired
# sudo systemctl start logstash
# sudo systemctl status logstash
